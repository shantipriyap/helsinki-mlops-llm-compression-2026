# LLM Compression: How Far Can We Go in Balancing Size and Performance?

This repository contains materials from the **Helsinki MLOps 2026** event, focusing on **large language model (LLM) compression and optimization**.

## Agenda

1. **Motivation and Challenge**  
   Why memory footprint, precision, and throughput matter for modern LLMs.

2. **Model Compression Techniques**  
   Approaches such as quantization (FP16 → FP8 → INT8 → INT4), pruning, and other optimization strategies.

3. **Use Case**  
   Practical examples demonstrating compression using **GPTQ** (Gradient-based Post-training Quantization) and **GSQ** (Group-wise Structured Quantization) to optimize model size and inference performance.

4. **Takeaways**  
   Key insights on memory-performance trade-offs, optimal precision choices, and deployment best practices.

## Repository Contents

- **Presentation Slides** – Covering the agenda above with charts and illustrations.  
- **Code Examples** – Demonstrating LLM compression, benchmarking, and deployment workflows using GPTQ and GSQ.  
- **References & Best Practices** – Guidelines for efficient LLM deployment in real-world MLOps pipelines.

## Audience

Designed for **ML engineers, researchers, and practitioners** interested in **efficient LLM workflows and performance optimization**.

## Citation / Source



